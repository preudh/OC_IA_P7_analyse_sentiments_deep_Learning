{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Développement du \"Modèle sur mesure avancé\" avec des essais sur au moins deux word embeddings différents et en gardant celui qui permet d’obtenir les meilleures performances. Nous utiliserons Glove et FastText.\n",
    "-  Nous allons construire et entraîner un réseau de neurones pour chaque embedding, évaluer les performances et enregistrer les expérimentations avec MLFlow. \n",
    "-  C'est la colonne clean_text_embeddings qui sera utilisée pour les embeddings."
   ],
   "id": "e130531f31fb4315"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Charger le fichier CSV nettoyé",
   "id": "3d87f719c46eedd4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T18:59:27.863809Z",
     "start_time": "2024-10-21T18:59:27.857393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verification version de tensorflow\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ],
   "id": "e1c42060620514e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T18:59:38.761636Z",
     "start_time": "2024-10-21T18:59:27.913228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Chemin relatif pour charger les données nettoyées\n",
    "file_path = os.path.join(\"..\", \"data\", \"cleaned_data_with_text_for_models.csv\")\n",
    "\n",
    "# Chargement du DataFrame nettoyé\n",
    "data = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "# Vérification du contenu du DataFrame"
   ],
   "id": "32e4a3afe9437cba",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T18:59:39.013149Z",
     "start_time": "2024-10-21T18:59:38.789604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Réinitialiser l'index pour récupérer toutes les colonnes, y compris 'text' si elle est utilisée comme index\n",
    "data = data.reset_index()\n",
    "\n",
    "# Vérifier les colonnes disponibles après réinitialisation de l'index\n",
    "print(data.columns)\n"
   ],
   "id": "96b0d02836e331ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'clean_text_tfidf', 'clean_text_embeddings', 'clean_text_bert',\n",
      "       'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T18:59:39.511500Z",
     "start_time": "2024-10-21T18:59:39.113420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataframe info\n",
    "data.info()"
   ],
   "id": "93044ff2a6fbce5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count    Dtype \n",
      "---  ------                 --------------    ----- \n",
      " 0   text                   1600000 non-null  object\n",
      " 1   clean_text_tfidf       1592857 non-null  object\n",
      " 2   clean_text_embeddings  1596608 non-null  object\n",
      " 3   clean_text_bert        1600000 non-null  object\n",
      " 4   target                 1600000 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 61.0+ MB\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T18:59:39.971406Z",
     "start_time": "2024-10-21T18:59:39.593880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Valeurs manquantes\n",
    "print(data.isnull().sum())"
   ],
   "id": "57ece347a263b0a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                        0\n",
      "clean_text_tfidf         7143\n",
      "clean_text_embeddings    3392\n",
      "clean_text_bert             0\n",
      "target                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T18:59:40.163787Z",
     "start_time": "2024-10-21T18:59:40.024709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filtrer les lignes avec des valeurs manquantes dans la colonne 'clean_text_embeddings'\n",
    "missing_embeddings = data[data['clean_text_embeddings'].isnull()]\n",
    "\n",
    "# Afficher deux exemples pour analyser les raisons des valeurs manquantes\n",
    "print(missing_embeddings[['text', 'clean_text_tfidf', 'clean_text_embeddings']].head(2))"
   ],
   "id": "9cc762165073be07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             text clean_text_tfidf clean_text_embeddings\n",
      "208    @mandayyy               NaN                   NaN\n",
      "249  @mandayyy                 NaN                   NaN\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T18:59:40.696101Z",
     "start_time": "2024-10-21T18:59:40.398991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Supprimer les lignes avec des valeurs manquantes dans la colonne 'clean_text_embeddings'\n",
    "data = data.dropna(subset=['clean_text_embeddings'])\n"
   ],
   "id": "d2889fed2b6fe5dd",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T18:59:41.033991Z",
     "start_time": "2024-10-21T18:59:40.722234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vérifier les valeurs manquantes après suppression\n",
    "print(data.isnull().sum())"
   ],
   "id": "c5c4e5dafe121336",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                        0\n",
      "clean_text_tfidf         3751\n",
      "clean_text_embeddings       0\n",
      "clean_text_bert             0\n",
      "target                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1 : Charger les embeddings GloVe et FastText\n",
    "- Les liens pour télécharger les embeddings GloVe et FastText sont les suivants :\n",
    "- https://nlp.stanford.edu/projects/glove/\n",
    "- https://fasttext.cc/docs/en/english-vectors.html\n"
   ],
   "id": "3361dd5221253fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:20.522111Z",
     "start_time": "2024-10-21T18:59:41.056496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "\n",
    "# Chemin relatif pour charger les fichiers d'embeddings\n",
    "glove_path = os.path.join(\"..\", \"data\", \"glove.twitter.27B.100d.txt\")\n",
    "fasttext_path = os.path.join(\"..\", \"data\", \"crawl-300d-2M-subword.bin\")\n",
    "\n",
    "# Charger les embeddings GloVe\n",
    "glove_model = KeyedVectors.load_word2vec_format(glove_path, binary=False, no_header=True)\n",
    "\n",
    "# Charger les embeddings FastText (format Facebook binaire)\n",
    "fasttext_model = load_facebook_vectors(fasttext_path)\n"
   ],
   "id": "c2d83fbbca438645",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2 : Créer une fonction pour générer une matrice d'embeddings\n",
    "- Créer une matrice d'embeddings pour chaque modèle, basée sur le vocabulaire des données textuelles."
   ],
   "id": "dc8f14eccfeb1737"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:20.549860Z",
     "start_time": "2024-10-21T19:02:20.544772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fonction pour créer la matrice d'embeddings\n",
    "def create_embedding_matrix(embedding_model, vocab, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(vocab) + 1, embedding_dim))\n",
    "    for word, i in vocab.items():\n",
    "        if word in embedding_model:\n",
    "            embedding_matrix[i] = embedding_model[word]\n",
    "        else:\n",
    "            # Si le mot n'est pas trouvé dans les embeddings, laisser un vecteur de zéros\n",
    "            embedding_matrix[i] = np.zeros(embedding_dim)\n",
    "    return embedding_matrix\n"
   ],
   "id": "6e1bcc9f182aba0b",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3 : Préparation des Données pour la vectorisation et la division en ensemble d'entrainement/test\n",
    "\n",
    "1. **Sous-échantillonnage des Données**\n",
    "2. **Définition des Paramètres de Tokenisation**\n",
    "3. **Application de `TextVectorization`**\n",
    "4. **Conversion des Textes en Séquences Numériques**\n",
    "5. **Préparation des Labels**\n",
    "6. **Division des Données en Ensemble d’Entraînement et de Test**"
   ],
   "id": "2c8665aac5a2fccf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:38.609092Z",
     "start_time": "2024-10-21T19:02:20.617178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# # Sous-échantillonnage des données à 0.1 % pour tester rapidement le pipeline sur 1 600 000 tweets soit 1600 tweets\n",
    "data_sample = data.sample(frac=0.001, random_state=42)\n",
    "\n",
    "# Tokenisation des textes nettoyés pour les word embeddings\n",
    "num_words = 10000 \n",
    "max_sequence_length = 100\n",
    "\n",
    "# Utiliser TextVectorization pour transformer les textes\n",
    "tv_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=num_words,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_sequence_length\n",
    ")\n",
    "\n",
    "# Adapter TextVectorization sur les textes\n",
    "tv_layer.adapt(data['clean_text_embeddings'])\n",
    "\n",
    "# Convertir les textes en séquences\n",
    "X = tv_layer(data['clean_text_embeddings'])\n",
    "\n",
    "# Convertir le Tensor en tableau NumPy\n",
    "X = X.numpy()\n",
    "\n",
    "# Préparer les labels\n",
    "y = data['target'].values  # Assurez-vous que y est également un tableau NumPy\n",
    "\n",
    "# Division des données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ],
   "id": "486307e6877516e0",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Créer et entraîner les modèles avec GloVe et FastText\n",
    "- Nous allons maintenant construire et entraîner les modèles avec les embeddings GloVe et FastText."
   ],
   "id": "2d17ebed06bbc8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- a) Construction du Modèle de Réseau de Neurones avec Embeddings Pré-entraînés :",
   "id": "4b61145740c0e51a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:38.635108Z",
     "start_time": "2024-10-21T19:02:38.628380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reconstruire la fonction build_model pour utiliser input_length différemment\n",
    "def build_model(embedding_matrix, input_length):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "    # Définir la taille des embeddings\n",
    "    vocab_size, embedding_dim = embedding_matrix.shape\n",
    "\n",
    "    # Créer le modèle\n",
    "    model = Sequential()\n",
    "\n",
    "    # Ajouter une couche d'embedding avec la matrice d'embedding pré-entraînée\n",
    "    model.add(Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False  # Ne pas entraîner l'embedding pré-entraîné\n",
    "    ))\n",
    "\n",
    "    # Ajouter d'autres couches, par exemple une couche LSTM\n",
    "    model.add(LSTM(64, input_shape=(input_length, embedding_dim)))\n",
    "\n",
    "    # Ajouter la couche de sortie\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compiler le modèle\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ],
   "id": "b6e4395a46e23f21",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- b) Entraînement du modèle avec GloVe :",
   "id": "712343f52d045a52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:46.605146Z",
     "start_time": "2024-10-21T19:02:38.654348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Créer la matrice d'embeddings pour GloVe\n",
    "def create_embedding_matrix(glove_model, tv_layer, embedding_dim):\n",
    "    # Récupérer l'index des mots de TextVectorization\n",
    "    word_index = {word: index for index, word in enumerate(tv_layer.get_vocabulary())}\n",
    "    \n",
    "    # Initialiser la matrice d'embeddings\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if word in glove_model:\n",
    "            embedding_vector = glove_model[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# Définir le nombre de mots et la longueur maximale des séquences\n",
    "num_words = 10000\n",
    "max_sequence_length = 100\n",
    "\n",
    "# Initialiser la couche TextVectorization\n",
    "tv_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=num_words,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_sequence_length\n",
    ")\n",
    "\n",
    "# Adapter TextVectorization sur les textes nettoyés\n",
    "tv_layer.adapt(data['clean_text_embeddings'])"
   ],
   "id": "50e4999f9be26bcd",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T23:56:08.067788Z",
     "start_time": "2024-10-21T19:02:46.684796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Définir la dimension des embeddings pour GloVe\n",
    "embedding_dim_glove = 100  # Taille des vecteurs d'embeddings pour GloVe, ajustez selon le modèle GloVe utilisé\n",
    "\n",
    "# Créer la matrice d'embeddings en utilisant TextVectorization (tv_layer)\n",
    "embedding_matrix_glove = create_embedding_matrix(glove_model, tv_layer, embedding_dim_glove)\n",
    "\n",
    "# Construire le modèle avec GloVe\n",
    "model_glove = build_model(embedding_matrix_glove, max_sequence_length)\n",
    "\n",
    "# Définir le chemin absolu pour le dossier \"models\"\n",
    "models_path = Path(\"../models\").resolve()\n",
    "best_model_path = models_path / 'best_model_glove.keras'\n",
    "\n",
    "# Définir un callback pour sauvegarder le meilleur modèle en fonction de la perte de validation\n",
    "checkpoint_cb = ModelCheckpoint(best_model_path, save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# Définir un callback pour arrêter l'entraînement si la perte de validation ne s'améliore pas après 3 époques\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Créer une liste de callbacks pour le modèle\n",
    "callbacks = [checkpoint_cb, early_stopping_cb]\n",
    "\n",
    "# Entraîner le modèle avec les callbacks\n",
    "history_glove = model_glove.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=8,  # Augmenter le nombre d'époques pour permettre un entraînement suffisant avec early stopping\n",
    "    batch_size=32, \n",
    "    validation_data=(X_test, y_test), \n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Récupérer la meilleure époque atteinte\n",
    "best_epoch_glove = early_stopping_cb.stopped_epoch - early_stopping_cb.patience + 1 if early_stopping_cb.stopped_epoch else 8\n",
    "print(f\"La meilleure époque pour GloVe est : {best_epoch_glove}\")\n",
    "\n",
    "#  Enregistrer les paramètres optimaux\n",
    "best_batch_size_glove =32\n",
    "best_epochs_glove = best_epoch_glove\n"
   ],
   "id": "4d2cb8b6023d8bce",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pat\\.conda\\envs\\P7te\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2132s\u001B[0m 53ms/step - accuracy: 0.6071 - loss: 0.6037 - val_accuracy: 0.8092 - val_loss: 0.4132\n",
      "Epoch 2/8\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2030s\u001B[0m 51ms/step - accuracy: 0.8138 - loss: 0.4055 - val_accuracy: 0.8165 - val_loss: 0.4015\n",
      "Epoch 3/8\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2230s\u001B[0m 56ms/step - accuracy: 0.8222 - loss: 0.3911 - val_accuracy: 0.8196 - val_loss: 0.3950\n",
      "Epoch 4/8\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2242s\u001B[0m 56ms/step - accuracy: 0.8264 - loss: 0.3825 - val_accuracy: 0.8205 - val_loss: 0.3944\n",
      "Epoch 5/8\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2237s\u001B[0m 56ms/step - accuracy: 0.8293 - loss: 0.3780 - val_accuracy: 0.8219 - val_loss: 0.3923\n",
      "Epoch 6/8\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2258s\u001B[0m 57ms/step - accuracy: 0.8317 - loss: 0.3737 - val_accuracy: 0.8223 - val_loss: 0.3924\n",
      "Epoch 7/8\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2215s\u001B[0m 55ms/step - accuracy: 0.8341 - loss: 0.3699 - val_accuracy: 0.8218 - val_loss: 0.3941\n",
      "Epoch 8/8\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2257s\u001B[0m 57ms/step - accuracy: 0.8349 - loss: 0.3680 - val_accuracy: 0.8209 - val_loss: 0.3935\n",
      "La meilleure époque pour GloVe est : 5\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- c) Entraînement du modèle avec FastText :",
   "id": "fcaf924a4ee54405"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T23:56:17.300510Z",
     "start_time": "2024-10-21T23:56:08.149272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Créer la matrice d'embeddings pour FastText\n",
    "def create_embedding_matrix(fasttext_model, tv_layer, embedding_dim):\n",
    "    # Récupérer l'index des mots de TextVectorization\n",
    "    word_index = {word: index for index, word in enumerate(tv_layer.get_vocabulary())}\n",
    "    \n",
    "    # Initialiser la matrice d'embeddings\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if word in fasttext_model:\n",
    "            embedding_vector = fasttext_model[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# Définir le nombre de mots et la longueur maximale des séquences\n",
    "num_words = 10000\n",
    "max_sequence_length = 100\n",
    "\n",
    "# Initialiser la couche TextVectorization\n",
    "tv_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=num_words,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_sequence_length\n",
    ")\n",
    "\n",
    "# Adapter TextVectorization sur les textes nettoyés\n",
    "tv_layer.adapt(data['clean_text_embeddings'])\n"
   ],
   "id": "9ed0385b946a3565",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T03:28:13.627027Z",
     "start_time": "2024-10-22T00:48:40.007479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Définir la dimension des embeddings pour FastText\n",
    "embedding_dim_fasttext = 300  # Assurez-vous que cela correspond à la dimension de vos vecteurs FastText\n",
    "\n",
    "# Créer la matrice d'embeddings en utilisant TextVectorization (tv_layer)\n",
    "embedding_matrix_fasttext = create_embedding_matrix(fasttext_model, tv_layer, embedding_dim_fasttext)\n",
    "\n",
    "# Construire le modèle avec FastText\n",
    "model_fasttext = build_model(embedding_matrix_fasttext, max_sequence_length)\n",
    "\n",
    "# Définir le chemin absolu pour le dossier \"models\"\n",
    "models_path = Path(\"../models\").resolve()\n",
    "best_model_path_fasttext = models_path / 'best_model_fasttext.keras'  # Utiliser l'extension .keras\n",
    "\n",
    "# Définir un callback pour sauvegarder le meilleur modèle en fonction de la perte de validation\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    best_model_path_fasttext,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_weights_only=False  # Assurez-vous que cette option est correctement configurée\n",
    ")\n",
    "\n",
    "# Définir un callback pour arrêter l'entraînement si la perte de validation ne s'améliore pas après 3 époques\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Créer une liste de callbacks pour le modèle\n",
    "callbacks = [checkpoint_cb, early_stopping_cb]\n",
    "\n",
    "# Entraîner le modèle avec les callbacks\n",
    "history_fasttext = model_fasttext.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,  # Augmenter le nombre d'époques pour permettre un entraînement suffisant avec early stopping\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Récupérer la meilleure époque atteinte\n",
    "best_epoch_fasttext = early_stopping_cb.stopped_epoch - early_stopping_cb.patience + 1 if early_stopping_cb.stopped_epoch else 5\n",
    "print(f\"La meilleure époque pour FastText est : {best_epoch_fasttext}\")\n",
    "\n",
    "# Enregistrer les paramètres optimaux\n",
    "best_batch_size_fasttext = 32  # Celui que vous avez utilisé\n",
    "best_epochs_fasttext = best_epoch_fasttext\n"
   ],
   "id": "e56614913e59e259",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pat\\.conda\\envs\\P7te\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1834s\u001B[0m 46ms/step - accuracy: 0.4999 - loss: 0.6932 - val_accuracy: 0.4997 - val_loss: 0.6931\n",
      "Epoch 2/5\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1759s\u001B[0m 44ms/step - accuracy: 0.5677 - loss: 0.6400 - val_accuracy: 0.7930 - val_loss: 0.4390\n",
      "Epoch 3/5\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1775s\u001B[0m 44ms/step - accuracy: 0.8069 - loss: 0.4178 - val_accuracy: 0.8103 - val_loss: 0.4103\n",
      "Epoch 4/5\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2059s\u001B[0m 52ms/step - accuracy: 0.8200 - loss: 0.3949 - val_accuracy: 0.8206 - val_loss: 0.3942\n",
      "Epoch 5/5\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2145s\u001B[0m 54ms/step - accuracy: 0.8258 - loss: 0.3840 - val_accuracy: 0.8235 - val_loss: 0.3879\n",
      "La meilleure époque pour FastText est : 5\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.Définition de la fonction evaluate_model pour évaluer les performances des modèles :",
   "id": "8fbb42da3c94c580"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Vérifier l'initialisation de MLFlow",
   "id": "b0075d9e232a4a49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T04:23:18.201709Z",
     "start_time": "2024-10-22T04:23:18.173345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "# Chemin absolu pour le dossier \"mlruns\"\n",
    "mlruns_path = Path(\"../mlruns\").resolve()\n",
    "\n",
    "# Vérifier que le dossier \"mlruns\" existe, sinon le créer\n",
    "if not mlruns_path.exists():\n",
    "    mlruns_path.mkdir(parents=True)\n",
    "\n",
    "# Vérifier que le sous-dossier \".trash\" existe, sinon le créer\n",
    "trash_folder = mlruns_path / \".trash\"\n",
    "if not trash_folder.exists():\n",
    "    trash_folder.mkdir(parents=True)\n",
    "\n",
    "# Définir le chemin avec le préfixe compatible Windows \"file:///\"\n",
    "mlflow_tracking_uri = f\"file:///{mlruns_path.as_posix()}\"\n",
    "\n",
    "# Définir l'URI de suivi dans MLflow\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "# Définir l'expérience MLFlow\n",
    "mlflow.set_experiment(\"text_classification_experiment\")\n",
    "\n",
    "\n"
   ],
   "id": "a5b4aa92969a179a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/22 06:23:18 INFO mlflow.tracking.fluent: Experiment with name 'text_classification_experiment' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///D:/OC_IA/P7/OC_IA_P7_analyse_sentiments_deep_Learning/mlruns/194210840617378958', creation_time=1729570998191, experiment_id='194210840617378958', last_update_time=1729570998191, lifecycle_stage='active', name='text_classification_experiment', tags={}>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fonction pour évaluer les performances du modèle :",
   "id": "6875e3409e2a45f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T05:38:35.848171Z",
     "start_time": "2024-10-22T05:38:35.838526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name, roc_curve_path):\n",
    "    # Prédictions du modèle\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Enregistrement des métriques dans MLFlow\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "\n",
    "    # Tracer et enregistrer la courbe ROC au chemin spécifié\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve for {model_name}')\n",
    "    plt.savefig(roc_curve_path)  # Sauvegarder la courbe ROC à l'emplacement spécifié\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Model evaluation for {model_name} completed.\")\n"
   ],
   "id": "5c5d29e0c92d5f4",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T04:58:26.468867Z",
     "start_time": "2024-10-22T04:58:26.460142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "# import matplotlib.pyplot as plt\n",
    "# import mlflow\n",
    "# import mlflow.keras\n",
    "# \n",
    "# def evaluate_model(model, X_test, y_test, model_name):\n",
    "#     # Prédictions du modèle\n",
    "#     y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "#     y_pred_proba = model.predict(X_test)\n",
    "#     \n",
    "#     # Calcul des métriques\n",
    "#     precision = precision_score(y_test, y_pred)\n",
    "#     recall = recall_score(y_test, y_pred)\n",
    "#     f1 = f1_score(y_test, y_pred)\n",
    "#     roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "# \n",
    "#     # Enregistrement des métriques dans MLFlow sans démarrer un nouveau run\n",
    "#     mlflow.log_metric(\"precision\", precision)\n",
    "#     mlflow.log_metric(\"recall\", recall)\n",
    "#     mlflow.log_metric(\"f1_score\", f1)\n",
    "#     mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "# \n",
    "#     # Tracer la courbe ROC\n",
    "#     fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "#     plt.figure()\n",
    "#     plt.plot(fpr, tpr, marker='.')\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title(f'ROC Curve for {model_name}')\n",
    "#     roc_curve_path = f\"roc_curve_{model_name}.png\"\n",
    "#     plt.savefig(roc_curve_path)\n",
    "#     mlflow.log_artifact(roc_curve_path)\n",
    "#     plt.close()\n",
    "# \n",
    "#     print(f\"Model evaluation for {model_name} completed.\")\n",
    "\n"
   ],
   "id": "53ff08b1bcfab0c1",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. Enregistrer les expérimentations avec MLFlow",
   "id": "5959befa2aba5fea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- a) Enregistrer les résultats pour GloVe :",
   "id": "3a8707b4cae59739"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T05:44:11.542816Z",
     "start_time": "2024-10-22T05:38:51.150575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "# Chemin absolu pour le dossier \"mlruns\"\n",
    "mlruns_path = Path(\"../mlruns\").resolve()\n",
    "\n",
    "# Vérifier que le dossier \"mlruns\" existe, sinon le créer\n",
    "if not mlruns_path.exists():\n",
    "    mlruns_path.mkdir(parents=True)\n",
    "\n",
    "# Vérifier que le sous-dossier \".trash\" existe, sinon le créer\n",
    "trash_folder = mlruns_path / \".trash\"\n",
    "if not trash_folder.exists():\n",
    "    trash_folder.mkdir(parents=True)\n",
    "\n",
    "# Configuration du chemin pour stocker les artefacts de MLFlow\n",
    "mlflow.set_tracking_uri(mlruns_path.as_uri())\n",
    "\n",
    "# Définir le nom de l'expérience\n",
    "experiment_name = \"GloVe_Embedding_Experiment\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Terminer tout run MLFlow actif s'il y en a un\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "# Démarrer une nouvelle session MLFlow pour GloVe\n",
    "with mlflow.start_run(run_name=\"GloVe_Model_Evaluation\", nested=True) as run:\n",
    "    # Enregistrer les paramètres du modèle\n",
    "    mlflow.log_param(\"embedding\", \"GloVe\")\n",
    "    mlflow.log_param(\"embedding_dim\", embedding_dim_glove)\n",
    "    mlflow.log_param(\"batch_size\", best_batch_size_glove)\n",
    "    mlflow.log_param(\"epochs\", best_epochs_glove)\n",
    "\n",
    "    # Récupérer l'ID du run actuel\n",
    "    run_id = run.info.run_id\n",
    "    experiment_id = run.info.experiment_id\n",
    "\n",
    "    # Construire le chemin pour enregistrer la courbe ROC dans le dossier du run\n",
    "    run_folder = mlruns_path / str(experiment_id) / str(run_id)\n",
    "    if not run_folder.exists():\n",
    "        run_folder.mkdir(parents=True)\n",
    "\n",
    "    roc_curve_path = run_folder / \"roc_curve_GloVe.png\"\n",
    "\n",
    "    # Évaluer le modèle et enregistrer les métriques et la courbe ROC au chemin spécifié\n",
    "    evaluate_model(model_glove, X_test, y_test, \"GloVe\", roc_curve_path)\n",
    "\n",
    "    # Enregistrer l'artefact dans MLFlow\n",
    "    mlflow.log_artifact(str(roc_curve_path))\n",
    "\n",
    "    # Enregistrer le modèle Keras avec MLFlow\n",
    "    mlflow.keras.log_model(model_glove, \"glove_model\")\n",
    "\n",
    "    # Ajouter des informations sur le run\n",
    "    mlflow.log_param(\"model_path\", \"glove_model\")\n",
    "    mlflow.log_param(\"experiment_id\", experiment_id)\n",
    "    mlflow.log_param(\"run_id\", run_id)\n",
    "\n",
    "print(\"Modèle GloVe évalué et enregistré dans MLFlow.\")\n"
   ],
   "id": "9020429fbcea46f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m9979/9979\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m151s\u001B[0m 15ms/step\n",
      "\u001B[1m9979/9979\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m153s\u001B[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/22 07:44:04 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation for GloVe completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/22 07:44:11 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle GloVe évalué et enregistré dans MLFlow.\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- b) Enregistrer les résultats pour FastText :",
   "id": "8cb066147e80c96b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T05:54:38.752800Z",
     "start_time": "2024-10-22T05:45:34.046794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "# Chemin absolu pour le dossier \"mlruns\"\n",
    "mlruns_path = Path(\"../mlruns\").resolve()\n",
    "\n",
    "# Vérifier que le dossier \"mlruns\" existe, sinon le créer\n",
    "if not mlruns_path.exists():\n",
    "    mlruns_path.mkdir(parents=True)\n",
    "\n",
    "# Vérifier que le sous-dossier \".trash\" existe, sinon le créer\n",
    "trash_folder = mlruns_path / \".trash\"\n",
    "if not trash_folder.exists():\n",
    "    trash_folder.mkdir(parents=True)\n",
    "\n",
    "# Configuration du chemin pour stocker les artefacts de MLFlow\n",
    "mlflow.set_tracking_uri(mlruns_path.as_uri())\n",
    "\n",
    "# Définir le nom de l'expérience\n",
    "experiment_name = \"FastText_Embedding_Experiment\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Terminer tout run MLFlow actif s'il y en a un\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "# Démarrer une nouvelle session MLFlow pour FastText\n",
    "with mlflow.start_run(run_name=\"FastText_Model_Evaluation\", nested=True) as run:\n",
    "    # Enregistrer les paramètres du modèle\n",
    "    mlflow.log_param(\"embedding\", \"FastText\")\n",
    "    mlflow.log_param(\"embedding_dim\", embedding_dim_fasttext)\n",
    "    mlflow.log_param(\"batch_size\", best_batch_size_fasttext)\n",
    "    mlflow.log_param(\"epochs\", best_epochs_fasttext)\n",
    "\n",
    "    # Récupérer l'ID du run actuel\n",
    "    run_id = run.info.run_id\n",
    "    experiment_id = run.info.experiment_id\n",
    "\n",
    "    # Construire le chemin pour enregistrer la courbe ROC dans le dossier du run\n",
    "    run_folder = mlruns_path / str(experiment_id) / str(run_id)\n",
    "    if not run_folder.exists():\n",
    "        run_folder.mkdir(parents=True)\n",
    "\n",
    "    roc_curve_path = run_folder / \"roc_curve_FastText.png\"\n",
    "\n",
    "    # Évaluer le modèle et enregistrer les métriques et la courbe ROC au chemin spécifié\n",
    "    evaluate_model(model_fasttext, X_test, y_test, \"FastText\", roc_curve_path)\n",
    "\n",
    "    # Enregistrer l'artefact dans MLFlow\n",
    "    mlflow.log_artifact(str(roc_curve_path))\n",
    "\n",
    "    # Enregistrer le modèle Keras avec MLFlow\n",
    "    mlflow.keras.log_model(model_fasttext, \"fasttext_model\")\n",
    "\n",
    "    # Ajouter des informations sur le run\n",
    "    mlflow.log_param(\"model_path\", \"fasttext_model\")\n",
    "    mlflow.log_param(\"experiment_id\", experiment_id)\n",
    "    mlflow.log_param(\"run_id\", run_id)\n",
    "\n",
    "print(\"Modèle FastText évalué et enregistré dans MLFlow.\")\n"
   ],
   "id": "c92d5af09e9449d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m9979/9979\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m209s\u001B[0m 21ms/step\n",
      "\u001B[1m9979/9979\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m272s\u001B[0m 27ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/22 07:53:57 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation for FastText completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/22 07:54:38 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle FastText évalué et enregistré dans MLFlow.\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Conclusion\n",
    "- Ce code utilise le fichier cleaned_data_with_text_for_models.csv et applique les word embeddings GloVe et FastText. Nous avons modifié le code pour utiliser les textes déjà nettoyés dans la colonne clean_text_embeddings. Après avoir entraîné les modèles, nous enregistrons les résultats et les modèles dans MLFlow afin de comparer les performances des deux embeddings et de sélectionner le meilleur."
   ],
   "id": "b16bb58980ad2dee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
