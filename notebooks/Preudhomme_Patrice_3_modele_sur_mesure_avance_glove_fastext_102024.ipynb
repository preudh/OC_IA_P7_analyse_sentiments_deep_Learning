{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Développement du \"Modèle sur mesure avancé\" avec des essais sur au moins deux word embeddings différents et en gardant celui qui permet d’obtenir les meilleures performances. Nous utiliserons Glove et FastText.\n",
    "-  Nous allons construire et entraîner un réseau de neurones pour chaque embedding, évaluer les performances et enregistrer les expérimentations avec MLFlow. \n",
    "-  C'est la colonne clean_text_embeddings qui sera utilisée pour les embeddings."
   ],
   "id": "e130531f31fb4315"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Charger le fichier CSV nettoyé",
   "id": "3d87f719c46eedd4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T07:26:51.218758Z",
     "start_time": "2024-10-23T07:26:51.213016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verification version de tensorflow\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ],
   "id": "e1c42060620514e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T07:27:04.203851Z",
     "start_time": "2024-10-23T07:26:54.803931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Chemin relatif pour charger les données nettoyées\n",
    "file_path = os.path.join(\"..\", \"data\", \"cleaned_data_with_text_for_models.csv\")\n",
    "\n",
    "# Chargement du DataFrame nettoyé\n",
    "data = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "# Vérification du contenu du DataFrame"
   ],
   "id": "32e4a3afe9437cba",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T07:27:18.425799Z",
     "start_time": "2024-10-23T07:27:18.264395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Réinitialiser l'index pour récupérer toutes les colonnes, y compris 'text' si elle est utilisée comme index\n",
    "data = data.reset_index()\n",
    "\n",
    "# Vérifier les colonnes disponibles après réinitialisation de l'index\n",
    "print(data.columns)\n"
   ],
   "id": "96b0d02836e331ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'clean_text_tfidf', 'clean_text_embeddings', 'clean_text_bert',\n",
      "       'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T07:27:21.599796Z",
     "start_time": "2024-10-23T07:27:21.340673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataframe info\n",
    "data.info()"
   ],
   "id": "93044ff2a6fbce5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count    Dtype \n",
      "---  ------                 --------------    ----- \n",
      " 0   text                   1600000 non-null  object\n",
      " 1   clean_text_tfidf       1592857 non-null  object\n",
      " 2   clean_text_embeddings  1596608 non-null  object\n",
      " 3   clean_text_bert        1600000 non-null  object\n",
      " 4   target                 1600000 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 61.0+ MB\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T07:27:24.847842Z",
     "start_time": "2024-10-23T07:27:24.594302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Valeurs manquantes\n",
    "print(data.isnull().sum())"
   ],
   "id": "57ece347a263b0a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                        0\n",
      "clean_text_tfidf         7143\n",
      "clean_text_embeddings    3392\n",
      "clean_text_bert             0\n",
      "target                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T07:27:28.935894Z",
     "start_time": "2024-10-23T07:27:28.872375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filtrer les lignes avec des valeurs manquantes dans la colonne 'clean_text_embeddings'\n",
    "missing_embeddings = data[data['clean_text_embeddings'].isnull()]\n",
    "\n",
    "# Afficher deux exemples pour analyser les raisons des valeurs manquantes\n",
    "print(missing_embeddings[['text', 'clean_text_tfidf', 'clean_text_embeddings']].head(2))"
   ],
   "id": "9cc762165073be07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             text clean_text_tfidf clean_text_embeddings\n",
      "208    @mandayyy               NaN                   NaN\n",
      "249  @mandayyy                 NaN                   NaN\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T07:27:32.262161Z",
     "start_time": "2024-10-23T07:27:32.000288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Supprimer les lignes avec des valeurs manquantes dans la colonne 'clean_text_embeddings'\n",
    "data = data.dropna(subset=['clean_text_embeddings'])\n"
   ],
   "id": "d2889fed2b6fe5dd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T07:27:35.325243Z",
     "start_time": "2024-10-23T07:27:35.092700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vérifier les valeurs manquantes après suppression\n",
    "print(data.isnull().sum())"
   ],
   "id": "c5c4e5dafe121336",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                        0\n",
      "clean_text_tfidf         3751\n",
      "clean_text_embeddings       0\n",
      "clean_text_bert             0\n",
      "target                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1 : Charger les embeddings GloVe et FastText\n",
    "- Les liens pour télécharger les embeddings GloVe et FastText sont les suivants :\n",
    "- https://nlp.stanford.edu/projects/glove/\n",
    "- https://fasttext.cc/docs/en/english-vectors.html\n"
   ],
   "id": "3361dd5221253fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T07:30:26.442732Z",
     "start_time": "2024-10-23T07:27:44.467233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "\n",
    "# Chemin relatif pour charger les fichiers d'embeddings\n",
    "glove_path = os.path.join(\"..\", \"data\", \"glove.twitter.27B.100d.txt\")\n",
    "fasttext_path = os.path.join(\"..\", \"data\", \"crawl-300d-2M-subword.bin\")\n",
    "\n",
    "# Charger les embeddings GloVe\n",
    "glove_model = KeyedVectors.load_word2vec_format(glove_path, binary=False, no_header=True)\n",
    "\n",
    "# Charger les embeddings FastText (format Facebook binaire)\n",
    "fasttext_model = load_facebook_vectors(fasttext_path)\n"
   ],
   "id": "c2d83fbbca438645",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2 : Créer une fonction pour générer une matrice d'embeddings\n",
    "- Créer une matrice d'embeddings pour chaque modèle, basée sur le vocabulaire des données textuelles."
   ],
   "id": "dc8f14eccfeb1737"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T07:35:13.116694Z",
     "start_time": "2024-10-23T07:35:13.110674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fonction pour créer la matrice d'embeddings\n",
    "def create_embedding_matrix(embedding_model, vocab, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(vocab) + 1, embedding_dim))\n",
    "    for word, i in vocab.items():\n",
    "        if word in embedding_model:\n",
    "            embedding_matrix[i] = embedding_model[word]\n",
    "        else:\n",
    "            # Si le mot n'est pas trouvé dans les embeddings, laisser un vecteur de zéros\n",
    "            embedding_matrix[i] = np.zeros(embedding_dim)\n",
    "    return embedding_matrix\n"
   ],
   "id": "6e1bcc9f182aba0b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3 : Préparation des Données pour la vectorisation et la division en ensemble d'entrainement/test\n",
    "\n",
    "1. **Sous-échantillonnage des Données**\n",
    "2. **Définition des Paramètres de Tokenisation**\n",
    "3. **Application de `TextVectorization`**\n",
    "4. **Conversion des Textes en Séquences Numériques**\n",
    "5. **Préparation des Labels**\n",
    "6. **Division des Données en Ensemble d’Entraînement et de Test**"
   ],
   "id": "2c8665aac5a2fccf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T07:35:48.910930Z",
     "start_time": "2024-10-23T07:35:30.862513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# # Sous-échantillonnage des données à 0.1 % pour tester rapidement le pipeline sur 1 600 000 tweets soit 1600 tweets\n",
    "data_sample = data.sample(frac=0.001, random_state=42)\n",
    "\n",
    "# Tokenisation des textes nettoyés pour les word embeddings\n",
    "num_words = 10000 \n",
    "max_sequence_length = 100\n",
    "\n",
    "# Utiliser TextVectorization pour transformer les textes\n",
    "tv_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=num_words,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_sequence_length\n",
    ")\n",
    "\n",
    "# Adapter TextVectorization sur les textes\n",
    "tv_layer.adapt(data['clean_text_embeddings'])\n",
    "\n",
    "# Convertir les textes en séquences\n",
    "X = tv_layer(data['clean_text_embeddings'])\n",
    "\n",
    "# Convertir le Tensor en tableau NumPy\n",
    "X = X.numpy()\n",
    "\n",
    "# Préparer les labels\n",
    "y = data['target'].values  # Assurez-vous que y est également un tableau NumPy\n",
    "\n",
    "# Division des données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ],
   "id": "486307e6877516e0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Créer et entraîner les modèles avec GloVe et FastText\n",
    "- Nous allons maintenant construire et entraîner les modèles avec les embeddings GloVe et FastText."
   ],
   "id": "2d17ebed06bbc8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- a) Construction du Modèle de Réseau de Neurones avec Embeddings Pré-entraînés :",
   "id": "4b61145740c0e51a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T07:37:02.724473Z",
     "start_time": "2024-10-23T07:37:02.716537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reconstruire la fonction build_model pour utiliser input_length différemment\n",
    "def build_model(embedding_matrix, input_length):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "    # Définir la taille des embeddings\n",
    "    vocab_size, embedding_dim = embedding_matrix.shape\n",
    "\n",
    "    # Créer le modèle\n",
    "    model = Sequential()\n",
    "\n",
    "    # Ajouter une couche d'embedding avec la matrice d'embedding pré-entraînée\n",
    "    model.add(Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False  # Ne pas entraîner l'embedding pré-entraîné\n",
    "    ))\n",
    "\n",
    "    # Ajouter d'autres couches, par exemple une couche LSTM\n",
    "    model.add(LSTM(64, input_shape=(input_length, embedding_dim)))\n",
    "\n",
    "    # Ajouter la couche de sortie\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compiler le modèle\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ],
   "id": "b6e4395a46e23f21",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- b) Entraînement du modèle avec GloVe :",
   "id": "712343f52d045a52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:46.605146Z",
     "start_time": "2024-10-21T19:02:38.654348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Créer la matrice d'embeddings pour GloVe\n",
    "def create_embedding_matrix(glove_model, tv_layer, embedding_dim):\n",
    "    # Récupérer l'index des mots de TextVectorization\n",
    "    word_index = {word: index for index, word in enumerate(tv_layer.get_vocabulary())}\n",
    "    \n",
    "    # Initialiser la matrice d'embeddings\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if word in glove_model:\n",
    "            embedding_vector = glove_model[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# Définir le nombre de mots et la longueur maximale des séquences\n",
    "num_words = 10000\n",
    "max_sequence_length = 100\n",
    "\n",
    "# Initialiser la couche TextVectorization\n",
    "tv_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=num_words,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_sequence_length\n",
    ")\n",
    "\n",
    "# Adapter TextVectorization sur les textes nettoyés\n",
    "tv_layer.adapt(data['clean_text_embeddings'])"
   ],
   "id": "50e4999f9be26bcd",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T23:56:08.067788Z",
     "start_time": "2024-10-21T19:02:46.684796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Définir la dimension des embeddings pour GloVe\n",
    "embedding_dim_glove = 100  # Taille des vecteurs d'embeddings pour GloVe, ajustez selon le modèle GloVe utilisé\n",
    "\n",
    "# Créer la matrice d'embeddings en utilisant TextVectorization (tv_layer)\n",
    "embedding_matrix_glove = create_embedding_matrix(glove_model, tv_layer, embedding_dim_glove)\n",
    "\n",
    "# Construire le modèle avec GloVe\n",
    "model_glove = build_model(embedding_matrix_glove, max_sequence_length)\n",
    "\n",
    "# Définir le chemin absolu pour le dossier \"models\"\n",
    "models_path = Path(\"../models\").resolve()\n",
    "best_model_path = models_path / 'best_model_glove.keras'\n",
    "\n",
    "# Définir un callback pour sauvegarder le meilleur modèle en fonction de la perte de validation\n",
    "checkpoint_cb = ModelCheckpoint(best_model_path, save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# Définir un callback pour arrêter l'entraînement si la perte de validation ne s'améliore pas après 3 époques\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Créer une liste de callbacks pour le modèle\n",
    "callbacks = [checkpoint_cb, early_stopping_cb]\n",
    "\n",
    "# Entraîner le modèle avec les callbacks\n",
    "history_glove = model_glove.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=8,  # Augmenter le nombre d'époques pour permettre un entraînement suffisant avec early stopping\n",
    "    batch_size=32, \n",
    "    validation_data=(X_test, y_test), \n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Récupérer la meilleure époque atteinte\n",
    "best_epoch_glove = early_stopping_cb.stopped_epoch - early_stopping_cb.patience + 1 if early_stopping_cb.stopped_epoch else 8\n",
    "print(f\"La meilleure époque pour GloVe est : {best_epoch_glove}\")\n",
    "\n",
    "#  Enregistrer les paramètres optimaux\n",
    "best_batch_size_glove =32\n",
    "best_epochs_glove = best_epoch_glove\n"
   ],
   "id": "4d2cb8b6023d8bce",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pat\\.conda\\envs\\P7te\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2132s\u001B[0m 53ms/step - accuracy: 0.6071 - loss: 0.6037 - val_accuracy: 0.8092 - val_loss: 0.4132\n",
      "Epoch 2/8\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2030s\u001B[0m 51ms/step - accuracy: 0.8138 - loss: 0.4055 - val_accuracy: 0.8165 - val_loss: 0.4015\n",
      "Epoch 3/8\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2230s\u001B[0m 56ms/step - accuracy: 0.8222 - loss: 0.3911 - val_accuracy: 0.8196 - val_loss: 0.3950\n",
      "Epoch 4/8\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2242s\u001B[0m 56ms/step - accuracy: 0.8264 - loss: 0.3825 - val_accuracy: 0.8205 - val_loss: 0.3944\n",
      "Epoch 5/8\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2237s\u001B[0m 56ms/step - accuracy: 0.8293 - loss: 0.3780 - val_accuracy: 0.8219 - val_loss: 0.3923\n",
      "Epoch 6/8\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2258s\u001B[0m 57ms/step - accuracy: 0.8317 - loss: 0.3737 - val_accuracy: 0.8223 - val_loss: 0.3924\n",
      "Epoch 7/8\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2215s\u001B[0m 55ms/step - accuracy: 0.8341 - loss: 0.3699 - val_accuracy: 0.8218 - val_loss: 0.3941\n",
      "Epoch 8/8\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2257s\u001B[0m 57ms/step - accuracy: 0.8349 - loss: 0.3680 - val_accuracy: 0.8209 - val_loss: 0.3935\n",
      "La meilleure époque pour GloVe est : 5\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- c) Entraînement du modèle avec FastText :",
   "id": "fcaf924a4ee54405"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T08:06:17.227906Z",
     "start_time": "2024-10-23T08:06:07.139918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Créer la matrice d'embeddings pour FastText\n",
    "def create_embedding_matrix(fasttext_model, tv_layer, embedding_dim):\n",
    "    word_index = {word: index for index, word in enumerate(tv_layer.get_vocabulary())}\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if word in fasttext_model:\n",
    "            embedding_vector = fasttext_model[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "# Définir le nombre de mots et la longueur maximale des séquences\n",
    "num_words = 10000\n",
    "max_sequence_length = 100\n",
    "\n",
    "# Initialiser la couche TextVectorization\n",
    "tv_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=num_words,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_sequence_length\n",
    ")\n",
    "\n",
    "# Adapter TextVectorization sur les textes nettoyés\n",
    "data['clean_text_embeddings'] = data['clean_text_embeddings'].apply(lambda x: x.encode('utf-8').decode('utf-8'))\n",
    "tv_layer.adapt(data['clean_text_embeddings'])\n",
    "\n",
    "# Sauvegarder la configuration de la couche\n",
    "config = tv_layer.get_config()\n",
    "vocabulary = tv_layer.get_vocabulary()\n",
    "\n",
    "# Assurez-vous que le répertoire \"models\" existe\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "# Sauvegarder la configuration et le vocabulaire\n",
    "with open(\"../models/tv_layer_config.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "with open(\"../models/tv_layer_vocabulary.txt\", \"w\", encoding='utf-8') as f:\n",
    "    for word in vocabulary:\n",
    "        f.write(f\"{word}\\n\")"
   ],
   "id": "8c0504ce00f8975e",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T07:47:44.236526Z",
     "start_time": "2024-10-23T07:47:34.794290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import pickle # Importer la bibliothèque pickle pour enregistrer les objets Python comme tv_layer afin de les réutiliser dans l'API pour la prédiction\n",
    "# \n",
    "# # Créer la matrice d'embeddings pour FastText\n",
    "# def create_embedding_matrix(fasttext_model, tv_layer, embedding_dim):\n",
    "#     # Récupérer l'index des mots de TextVectorization\n",
    "#     word_index = {word: index for index, word in enumerate(tv_layer.get_vocabulary())}\n",
    "#     \n",
    "#     # Initialiser la matrice d'embeddings\n",
    "#     embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "#     for word, i in word_index.items():\n",
    "#         if word in fasttext_model:\n",
    "#             embedding_vector = fasttext_model[word]\n",
    "#             embedding_matrix[i] = embedding_vector\n",
    "#     return embedding_matrix\n",
    "# \n",
    "# # Définir le nombre de mots et la longueur maximale des séquences\n",
    "# num_words = 10000\n",
    "# max_sequence_length = 100\n",
    "# \n",
    "# # Initialiser la couche TextVectorization\n",
    "# tv_layer = tf.keras.layers.TextVectorization(\n",
    "#     max_tokens=num_words,\n",
    "#     output_mode='int',\n",
    "#     output_sequence_length=max_sequence_length\n",
    "# )\n",
    "# \n",
    "# # Adapter TextVectorization sur les textes nettoyés\n",
    "# tv_layer.adapt(data['clean_text_embeddings'])\n",
    "# \n",
    "# \n",
    "# # Sauvegarder la couche `tv_layer` pour une utilisation ultérieure dans l'API. Si cela n'est pas fait alors l'API ne pourra pas être utilisée pour la prédiction. Il faut donc sauvegarder la couche `tv_layer` pour une au moment de l'entraînement du modèle.\n",
    "# \n",
    "# with open(\"../models/tv_layer.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(tv_layer, file)"
   ],
   "id": "9ed0385b946a3565",
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\x99' in position 3194: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeEncodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 36\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Sauvegarder la couche `tv_layer` pour une utilisation ultérieure dans l'API. Si cela n'est pas fait alors l'API ne pourra pas être utilisée pour la prédiction. Il faut donc sauvegarder la couche `tv_layer` pour une au moment de l'entraînement du modèle.\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../models/tv_layer.pkl\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[1;32m---> 36\u001B[0m     \u001B[43mpickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtv_layer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P7te\\Lib\\site-packages\\keras\\src\\saving\\keras_saveable.py:34\u001B[0m, in \u001B[0;36mKerasSaveable.__reduce__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving_lib\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msaving_lib\u001B[39;00m\n\u001B[0;32m     33\u001B[0m buf \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mBytesIO()\n\u001B[1;32m---> 34\u001B[0m \u001B[43msaving_lib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save_model_to_fileobj\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mh5\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_unpickle_model,\n\u001B[0;32m     37\u001B[0m     (buf,),\n\u001B[0;32m     38\u001B[0m )\n",
      "File \u001B[1;32m~\\.conda\\envs\\P7te\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:245\u001B[0m, in \u001B[0;36m_save_model_to_fileobj\u001B[1;34m(model, fileobj, weights_format)\u001B[0m\n\u001B[0;32m    237\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    238\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown `weights_format` argument. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    239\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mh5\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnpz\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    240\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived: weights_format=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mweights_format\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    241\u001B[0m         )\n\u001B[0;32m    243\u001B[0m     asset_store \u001B[38;5;241m=\u001B[39m DiskIOStore(_ASSETS_DIRNAME, archive\u001B[38;5;241m=\u001B[39mzf, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 245\u001B[0m     \u001B[43m_save_state\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    246\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    247\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweights_store\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights_store\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    248\u001B[0m \u001B[43m        \u001B[49m\u001B[43massets_store\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43masset_store\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    249\u001B[0m \u001B[43m        \u001B[49m\u001B[43minner_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    250\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvisited_saveables\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mset\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    251\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    252\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m    253\u001B[0m     \u001B[38;5;66;03m# Skip the final `zf.write` if any exception is raised\u001B[39;00m\n\u001B[0;32m    254\u001B[0m     write_zf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P7te\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:671\u001B[0m, in \u001B[0;36m_save_state\u001B[1;34m(saveable, weights_store, assets_store, inner_path, visited_saveables)\u001B[0m\n\u001B[0;32m    667\u001B[0m     saveable\u001B[38;5;241m.\u001B[39msave_own_variables(\n\u001B[0;32m    668\u001B[0m         weights_store\u001B[38;5;241m.\u001B[39mmake(inner_path, metadata\u001B[38;5;241m=\u001B[39mmetadata)\n\u001B[0;32m    669\u001B[0m     )\n\u001B[0;32m    670\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(saveable, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msave_assets\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m assets_store:\n\u001B[1;32m--> 671\u001B[0m     \u001B[43msaveable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_assets\u001B[49m\u001B[43m(\u001B[49m\u001B[43massets_store\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake\u001B[49m\u001B[43m(\u001B[49m\u001B[43minner_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    673\u001B[0m visited_saveables\u001B[38;5;241m.\u001B[39madd(\u001B[38;5;28mid\u001B[39m(saveable))\n\u001B[0;32m    675\u001B[0m \u001B[38;5;66;03m# Recursively save state of children saveables (layers, optimizers, etc.)\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P7te\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\text_vectorization.py:623\u001B[0m, in \u001B[0;36mTextVectorization.save_assets\u001B[1;34m(self, dir_path)\u001B[0m\n\u001B[0;32m    622\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msave_assets\u001B[39m(\u001B[38;5;28mself\u001B[39m, dir_path):\n\u001B[1;32m--> 623\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_lookup_layer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_assets\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdir_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P7te\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\index_lookup.py:823\u001B[0m, in \u001B[0;36mIndexLookup.save_assets\u001B[1;34m(self, dir_path)\u001B[0m\n\u001B[0;32m    821\u001B[0m vocabulary_filepath \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mio\u001B[38;5;241m.\u001B[39mgfile\u001B[38;5;241m.\u001B[39mjoin(dir_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvocabulary.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    822\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(vocabulary_filepath, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m--> 823\u001B[0m     \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mvocabulary\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P7te\\Lib\\encodings\\cp1252.py:19\u001B[0m, in \u001B[0;36mIncrementalEncoder.encode\u001B[1;34m(self, input, final)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mencode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m---> 19\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcodecs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcharmap_encode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43mencoding_table\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mUnicodeEncodeError\u001B[0m: 'charmap' codec can't encode character '\\x99' in position 3194: character maps to <undefined>"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T03:28:13.627027Z",
     "start_time": "2024-10-22T00:48:40.007479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Définir la dimension des embeddings pour FastText\n",
    "embedding_dim_fasttext = 300  # Assurez-vous que cela correspond à la dimension de vos vecteurs FastText\n",
    "\n",
    "# Créer la matrice d'embeddings en utilisant TextVectorization (tv_layer)\n",
    "embedding_matrix_fasttext = create_embedding_matrix(fasttext_model, tv_layer, embedding_dim_fasttext)\n",
    "\n",
    "# Construire le modèle avec FastText\n",
    "model_fasttext = build_model(embedding_matrix_fasttext, max_sequence_length)\n",
    "\n",
    "# Définir le chemin absolu pour le dossier \"models\"\n",
    "models_path = Path(\"../models\").resolve()\n",
    "best_model_path_fasttext = models_path / 'best_model_fasttext.keras'  # Utiliser l'extension .keras\n",
    "\n",
    "# Définir un callback pour sauvegarder le meilleur modèle en fonction de la perte de validation\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    best_model_path_fasttext,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_weights_only=False  # Assurez-vous que cette option est correctement configurée\n",
    ")\n",
    "\n",
    "# Définir un callback pour arrêter l'entraînement si la perte de validation ne s'améliore pas après 3 époques\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Créer une liste de callbacks pour le modèle\n",
    "callbacks = [checkpoint_cb, early_stopping_cb]\n",
    "\n",
    "# Entraîner le modèle avec les callbacks\n",
    "history_fasttext = model_fasttext.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,  # Augmenter le nombre d'époques pour permettre un entraînement suffisant avec early stopping\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Récupérer la meilleure époque atteinte\n",
    "best_epoch_fasttext = early_stopping_cb.stopped_epoch - early_stopping_cb.patience + 1 if early_stopping_cb.stopped_epoch else 5\n",
    "print(f\"La meilleure époque pour FastText est : {best_epoch_fasttext}\")\n",
    "\n",
    "# Enregistrer les paramètres optimaux\n",
    "best_batch_size_fasttext = 32  # Celui que vous avez utilisé\n",
    "best_epochs_fasttext = best_epoch_fasttext\n"
   ],
   "id": "e56614913e59e259",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pat\\.conda\\envs\\P7te\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1834s\u001B[0m 46ms/step - accuracy: 0.4999 - loss: 0.6932 - val_accuracy: 0.4997 - val_loss: 0.6931\n",
      "Epoch 2/5\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1759s\u001B[0m 44ms/step - accuracy: 0.5677 - loss: 0.6400 - val_accuracy: 0.7930 - val_loss: 0.4390\n",
      "Epoch 3/5\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1775s\u001B[0m 44ms/step - accuracy: 0.8069 - loss: 0.4178 - val_accuracy: 0.8103 - val_loss: 0.4103\n",
      "Epoch 4/5\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2059s\u001B[0m 52ms/step - accuracy: 0.8200 - loss: 0.3949 - val_accuracy: 0.8206 - val_loss: 0.3942\n",
      "Epoch 5/5\n",
      "\u001B[1m39916/39916\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2145s\u001B[0m 54ms/step - accuracy: 0.8258 - loss: 0.3840 - val_accuracy: 0.8235 - val_loss: 0.3879\n",
      "La meilleure époque pour FastText est : 5\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.Définition de la fonction evaluate_model pour évaluer les performances des modèles :",
   "id": "8fbb42da3c94c580"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Vérifier l'initialisation de MLFlow",
   "id": "b0075d9e232a4a49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T04:23:18.201709Z",
     "start_time": "2024-10-22T04:23:18.173345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "# Chemin absolu pour le dossier \"mlruns\"\n",
    "mlruns_path = Path(\"../mlruns\").resolve()\n",
    "\n",
    "# Vérifier que le dossier \"mlruns\" existe, sinon le créer\n",
    "if not mlruns_path.exists():\n",
    "    mlruns_path.mkdir(parents=True)\n",
    "\n",
    "# Vérifier que le sous-dossier \".trash\" existe, sinon le créer\n",
    "trash_folder = mlruns_path / \".trash\"\n",
    "if not trash_folder.exists():\n",
    "    trash_folder.mkdir(parents=True)\n",
    "\n",
    "# Définir le chemin avec le préfixe compatible Windows \"file:///\"\n",
    "mlflow_tracking_uri = f\"file:///{mlruns_path.as_posix()}\"\n",
    "\n",
    "# Définir l'URI de suivi dans MLflow\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "# Définir l'expérience MLFlow\n",
    "mlflow.set_experiment(\"text_classification_experiment\")\n",
    "\n",
    "\n"
   ],
   "id": "a5b4aa92969a179a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/22 06:23:18 INFO mlflow.tracking.fluent: Experiment with name 'text_classification_experiment' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///D:/OC_IA/P7/OC_IA_P7_analyse_sentiments_deep_Learning/mlruns/194210840617378958', creation_time=1729570998191, experiment_id='194210840617378958', last_update_time=1729570998191, lifecycle_stage='active', name='text_classification_experiment', tags={}>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fonction pour évaluer les performances du modèle :",
   "id": "6875e3409e2a45f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T05:38:35.848171Z",
     "start_time": "2024-10-22T05:38:35.838526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name, roc_curve_path):\n",
    "    # Prédictions du modèle\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Enregistrement des métriques dans MLFlow\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "\n",
    "    # Tracer et enregistrer la courbe ROC au chemin spécifié\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve for {model_name}')\n",
    "    plt.savefig(roc_curve_path)  # Sauvegarder la courbe ROC à l'emplacement spécifié\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Model evaluation for {model_name} completed.\")\n"
   ],
   "id": "5c5d29e0c92d5f4",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. Enregistrer les expérimentations avec MLFlow",
   "id": "5959befa2aba5fea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- a) Enregistrer les résultats pour GloVe :",
   "id": "3a8707b4cae59739"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T05:44:11.542816Z",
     "start_time": "2024-10-22T05:38:51.150575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "# Chemin absolu pour le dossier \"mlruns\"\n",
    "mlruns_path = Path(\"../mlruns\").resolve()\n",
    "\n",
    "# Vérifier que le dossier \"mlruns\" existe, sinon le créer\n",
    "if not mlruns_path.exists():\n",
    "    mlruns_path.mkdir(parents=True)\n",
    "\n",
    "# Vérifier que le sous-dossier \".trash\" existe, sinon le créer\n",
    "trash_folder = mlruns_path / \".trash\"\n",
    "if not trash_folder.exists():\n",
    "    trash_folder.mkdir(parents=True)\n",
    "\n",
    "# Configuration du chemin pour stocker les artefacts de MLFlow\n",
    "mlflow.set_tracking_uri(mlruns_path.as_uri())\n",
    "\n",
    "# Définir le nom de l'expérience\n",
    "experiment_name = \"GloVe_Embedding_Experiment\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Terminer tout run MLFlow actif s'il y en a un\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "# Démarrer une nouvelle session MLFlow pour GloVe\n",
    "with mlflow.start_run(run_name=\"GloVe_Model_Evaluation\", nested=True) as run:\n",
    "    # Enregistrer les paramètres du modèle\n",
    "    mlflow.log_param(\"embedding\", \"GloVe\")\n",
    "    mlflow.log_param(\"embedding_dim\", embedding_dim_glove)\n",
    "    mlflow.log_param(\"batch_size\", best_batch_size_glove)\n",
    "    mlflow.log_param(\"epochs\", best_epochs_glove)\n",
    "\n",
    "    # Récupérer l'ID du run actuel\n",
    "    run_id = run.info.run_id\n",
    "    experiment_id = run.info.experiment_id\n",
    "\n",
    "    # Construire le chemin pour enregistrer la courbe ROC dans le dossier du run\n",
    "    run_folder = mlruns_path / str(experiment_id) / str(run_id)\n",
    "    if not run_folder.exists():\n",
    "        run_folder.mkdir(parents=True)\n",
    "\n",
    "    roc_curve_path = run_folder / \"roc_curve_GloVe.png\"\n",
    "\n",
    "    # Évaluer le modèle et enregistrer les métriques et la courbe ROC au chemin spécifié\n",
    "    evaluate_model(model_glove, X_test, y_test, \"GloVe\", roc_curve_path)\n",
    "\n",
    "    # Enregistrer l'artefact dans MLFlow\n",
    "    mlflow.log_artifact(str(roc_curve_path))\n",
    "\n",
    "    # Enregistrer le modèle Keras avec MLFlow\n",
    "    mlflow.keras.log_model(model_glove, \"glove_model\")\n",
    "\n",
    "    # Ajouter des informations sur le run\n",
    "    mlflow.log_param(\"model_path\", \"glove_model\")\n",
    "    mlflow.log_param(\"experiment_id\", experiment_id)\n",
    "    mlflow.log_param(\"run_id\", run_id)\n",
    "\n",
    "print(\"Modèle GloVe évalué et enregistré dans MLFlow.\")\n"
   ],
   "id": "9020429fbcea46f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m9979/9979\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m151s\u001B[0m 15ms/step\n",
      "\u001B[1m9979/9979\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m153s\u001B[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/22 07:44:04 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation for GloVe completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/22 07:44:11 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle GloVe évalué et enregistré dans MLFlow.\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- b) Enregistrer les résultats pour FastText :",
   "id": "8cb066147e80c96b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T05:54:38.752800Z",
     "start_time": "2024-10-22T05:45:34.046794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "# Chemin absolu pour le dossier \"mlruns\"\n",
    "mlruns_path = Path(\"../mlruns\").resolve()\n",
    "\n",
    "# Vérifier que le dossier \"mlruns\" existe, sinon le créer\n",
    "if not mlruns_path.exists():\n",
    "    mlruns_path.mkdir(parents=True)\n",
    "\n",
    "# Vérifier que le sous-dossier \".trash\" existe, sinon le créer\n",
    "trash_folder = mlruns_path / \".trash\"\n",
    "if not trash_folder.exists():\n",
    "    trash_folder.mkdir(parents=True)\n",
    "\n",
    "# Configuration du chemin pour stocker les artefacts de MLFlow\n",
    "mlflow.set_tracking_uri(mlruns_path.as_uri())\n",
    "\n",
    "# Définir le nom de l'expérience\n",
    "experiment_name = \"FastText_Embedding_Experiment\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Terminer tout run MLFlow actif s'il y en a un\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "# Démarrer une nouvelle session MLFlow pour FastText\n",
    "with mlflow.start_run(run_name=\"FastText_Model_Evaluation\", nested=True) as run:\n",
    "    # Enregistrer les paramètres du modèle\n",
    "    mlflow.log_param(\"embedding\", \"FastText\")\n",
    "    mlflow.log_param(\"embedding_dim\", embedding_dim_fasttext)\n",
    "    mlflow.log_param(\"batch_size\", best_batch_size_fasttext)\n",
    "    mlflow.log_param(\"epochs\", best_epochs_fasttext)\n",
    "\n",
    "    # Récupérer l'ID du run actuel\n",
    "    run_id = run.info.run_id\n",
    "    experiment_id = run.info.experiment_id\n",
    "\n",
    "    # Construire le chemin pour enregistrer la courbe ROC dans le dossier du run\n",
    "    run_folder = mlruns_path / str(experiment_id) / str(run_id)\n",
    "    if not run_folder.exists():\n",
    "        run_folder.mkdir(parents=True)\n",
    "\n",
    "    roc_curve_path = run_folder / \"roc_curve_FastText.png\"\n",
    "\n",
    "    # Évaluer le modèle et enregistrer les métriques et la courbe ROC au chemin spécifié\n",
    "    evaluate_model(model_fasttext, X_test, y_test, \"FastText\", roc_curve_path)\n",
    "\n",
    "    # Enregistrer l'artefact dans MLFlow\n",
    "    mlflow.log_artifact(str(roc_curve_path))\n",
    "\n",
    "    # Enregistrer le modèle Keras avec MLFlow\n",
    "    mlflow.keras.log_model(model_fasttext, \"fasttext_model\")\n",
    "\n",
    "    # Ajouter des informations sur le run\n",
    "    mlflow.log_param(\"model_path\", \"fasttext_model\")\n",
    "    mlflow.log_param(\"experiment_id\", experiment_id)\n",
    "    mlflow.log_param(\"run_id\", run_id)\n",
    "\n",
    "print(\"Modèle FastText évalué et enregistré dans MLFlow.\")\n"
   ],
   "id": "c92d5af09e9449d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m9979/9979\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m209s\u001B[0m 21ms/step\n",
      "\u001B[1m9979/9979\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m272s\u001B[0m 27ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/22 07:53:57 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation for FastText completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/22 07:54:38 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle FastText évalué et enregistré dans MLFlow.\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Conclusion\n",
    "- Ce code utilise le fichier cleaned_data_with_text_for_models.csv et applique les word embeddings GloVe et FastText. Nous avons modifié le code pour utiliser les textes déjà nettoyés dans la colonne clean_text_embeddings. Après avoir entraîné les modèles, nous enregistrons les résultats et les modèles dans MLFlow afin de comparer les performances des deux embeddings et de sélectionner le meilleur.\n",
    "- \n",
    "- Pour déterminer le meilleur modèle entre GloVe et FastText à partir des métriques dans MLflow, voici les points principaux à analyser :\n",
    "\n",
    "1. **F1-Score** : C'est une métrique clé qui combine précision et rappel. Un score plus proche de 1 indique un meilleur équilibre entre ces deux mesures.\n",
    "2. **Précision et Rappel** : Regarder si un modèle a des valeurs nettement plus élevées pour ces métriques.\n",
    "3. **ROC AUC** : Indique la capacité du modèle à distinguer les classes. Un score plus proche de 1 est idéal.\n",
    "\n",
    "D’après mlflow, les valeurs sont similaires, mais FastText montre une légère supériorité sur le F1-Score et le ROC AUC. Cela indiquerait que **FastText** est potentiellement le meilleur modèle, nous allons donc retenir ce modèle pour la suite de nos travaux et l'utlisation dans le cadre de l'API qui sera développée pour la prédiction de la polarité des tweets avec FastText, FastAPI dans un premier temps en local.  "
   ],
   "id": "b16bb58980ad2dee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "8.**Pour intégrer le modèle déjà entraîné avec FastAPI en local**, voici les étapes à suivre :\n",
    "\n",
    "Exporter le modèle : Utilisez le modèle Keras enregistré (best_model_fasttext.keras).\n",
    "\n",
    "Créer un dossier API : Dans PyCharm, créez un nouveau dossier pour votre API, par exemple, api/.\n",
    "\n",
    "Créer un fichier principal FastAPI (main.py) dans ce dossier :"
   ],
   "id": "ae9ea82d0815421"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "df2d236e7ad7cce4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
